{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alive-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTM_Model\n",
    "from data import init_datasets\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "guilty-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'nyt_covid'\n",
    "path = 'data/small_test_corpora/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fossil-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "time_steps = 5\n",
    "freq_threshold = 1\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "professional-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = init_datasets(topic=topic, \n",
    "                         freq_threshold=freq_threshold, \n",
    "                         time_steps=time_steps, \n",
    "                         batch_size=batch_size, \n",
    "                         path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "contemporary-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = datasets['vocab_size']\n",
    "word2index = datasets['word2index']\n",
    "data_loaders = datasets['data_loaders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "electrical-northeast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bored-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "finnish-southwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = next(iter(train))\n",
    "x[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "turned-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 4, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dataset.x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inner-translator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[38],\n",
       "        [13],\n",
       "        [26]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "residential-premium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13],\n",
       "        [26],\n",
       "        [ 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1][0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fifteen-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'embed_dims': None,\n",
    "    'freq_threshold': 3,\n",
    "    'dropout_prob': 0.5,\n",
    "    'init_range': 0.05,\n",
    "    'epochs': 40,\n",
    "    'learning_rate': 1,\n",
    "    'learning_rate_decay': 1.2,\n",
    "    'num_layers': 2,\n",
    "    'batch_size': 20,\n",
    "    'time_steps': 35,\n",
    "    'max_grad': 5,\n",
    "    'embed_tying': False,\n",
    "    'bias': False,\n",
    "    'save_model': True,\n",
    "    'load_model': True,\n",
    "    'model_path': 'lstm_model',\n",
    "    'topic': 'wiki', # enter 'wiki' or 'nyt_covid'\n",
    "    'path': 'data/small_test_corpora'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "other-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLD_LUKE_train(model, data, epochs, learning_rate, learning_rate_decay, max_grad):\n",
    "    train_loader, valid_loader, test_loader = data\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"Starting training.\\n\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_size = train_loader.batch_size\n",
    "        states = model.init_state(batch_size)\n",
    "\n",
    "        if epoch > 5:\n",
    "            learning_rate = learning_rate / learning_rate_decay\n",
    "\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            batch_size = len(x)\n",
    "            model.zero_grad()\n",
    "            states = model.detach_states(states)\n",
    "            scores, states = model(x, states)\n",
    "            loss = neg_log_likelihood_loss(scores, y)\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad)\n",
    "                for param in model.parameters():\n",
    "                    param -= learning_rate * param.grad\n",
    "            if i % (len(train_data) // 10) == 0:\n",
    "                end_time = time.time()\n",
    "                print(\"batch no = {:d} / {:d}, \".format(i, len(train_loader)) +\n",
    "                      \"train loss = {:.3f}, \".format(loss.item() / batch_size) +\n",
    "                      \"dw.norm() = {:.3f}, \".format(norm) +\n",
    "                      \"lr = {:.3f}, \".format(learning_rate) +\n",
    "                      \"since beginning = {:d} mins, \".format(round((end_time-start_time)/60)) +\n",
    "                      \"cuda memory = {:.3f} GBs\".format(torch.cuda.max_memory_allocated()/1024/1024/1024))\n",
    "        model.eval()\n",
    "        valid_perplexity = get_perplexity(model, valid_loader, batch_size)\n",
    "        print(\"Epoch : {:d} || Validation set perplexity : {:.3f}\".format(epoch+1, valid_perplexity))\n",
    "        print(\"*************************************************\\n\")\n",
    "    test_perp = get_perplexity(model, test_loader, batch_size)\n",
    "    print(\"Test set perplexity : {:.3f}\".format(test_perp))\n",
    "    print(\"Training is over.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "legitimate-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The loss function.\n",
    "def _neg_log_likelihood_loss(scores, y):\n",
    "    batch_size = y.size(1)\n",
    "    print(\"y shape: \", y.shape)\n",
    "    print(\"scores shape: \", scores.shape)\n",
    "    expscores = scores.exp()\n",
    "    print(\"expscores shape: \", expscores.shape)\n",
    "    probabilities = expscores / expscores.sum(1, keepdim = True)\n",
    "    print(\"prob shape: \", probabilities.shape)\n",
    "    print(\"dim 1 : \", len(y.reshape(-1)))\n",
    "    print(\"dim 2 : \", y.reshape(-1).shape)\n",
    "    answerprobs = probabilities[range(len(y.reshape(-1))), y.reshape(-1)]\n",
    "    #I multiply by batch_size as in the original paper\n",
    "    #Zaremba et al. sum the loss over batches but average these over time.\n",
    "    return torch.mean(-torch.log(answerprobs) * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "burning-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood_loss(scores, targets):\n",
    "    # substituting with cross entropy loss\n",
    "    batch_size = targets.size(1)\n",
    "    return F.cross_entropy(scores.reshape(-1, scores.size(2)), targets.reshape(-1)) * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "packed-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(model, data, batch_size):\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        states = model.init_state(batch_size)\n",
    "        for x, y in data:\n",
    "            # x = torch.transpose(x, 0, 1)\n",
    "            # y = torch.transpose(y, 0, 1)\n",
    "            scores, states = model(x, states)\n",
    "            loss = neg_log_likelihood_loss(scores, y)\n",
    "            #Again with the sum/average implementation described in 'nll_loss'.\n",
    "            losses.append(loss.data.item() / batch_size)\n",
    "    return np.exp(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mature-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def train(device, data, model, epochs, lr, max_grad, batch_size):\n",
    "    train_loader, valid_loader, test_loader = data\n",
    "    tic = timeit.default_timer()\n",
    "    total_words = 0\n",
    "    print(\"Starting training.\\n\")\n",
    "    for epoch in range(epochs):\n",
    "        states = model.init_state(batch_size)\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            if x.size(0) < batch_size:\n",
    "                continue\n",
    "            # x = torch.transpose(x, 0, 1)\n",
    "            # y = torch.transpose(y, 0, 1)\n",
    "            total_words += x.numel()\n",
    "            model.zero_grad()\n",
    "            states = model.detach_states(states)\n",
    "            scores, states = model(x, states)\n",
    "            loss = neg_log_likelihood_loss(scores, y)\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                norm = nn.utils.clip_grad_norm_(model.parameters(), max_grad)\n",
    "                for param in model.parameters():\n",
    "                    param -= lr * param.grad\n",
    "            if i % (len(train_loader)//10) == 0:\n",
    "                toc = timeit.default_timer()\n",
    "                print(\"batch no = {:d} / {:d}, \".format(i, len(train_loader)) +\n",
    "                      \"train loss = {:.3f}, \".format(loss.item()/batch_size) +\n",
    "                      \"wps = {:d}, \".format(round(total_words/(toc-tic))) +\n",
    "                      \"dw.norm() = {:.3f}, \".format(norm) +\n",
    "                      \"lr = {:.3f}, \".format(lr) +\n",
    "                      \"since beginning = {:d} mins, \".format(round((toc-tic)/60))) \n",
    "                    #   \"cuda memory = {:.3f} GBs\".format(torch.cuda.max_memory_allocated()/1024/1024/1024))\n",
    "    \n",
    "    return model\n",
    "    #     model.eval()\n",
    "    #     val_perp = perplexity(vld, model)\n",
    "    #     print(\"Epoch : {:d} || Validation set perplexity : {:.3f}\".format(epoch+1, val_perp))\n",
    "    #     print(\"*************************************************\\n\")\n",
    "    # tst_perp = perplexity(tst, model)\n",
    "    # print(\"Test set perplexity : {:.3f}\".format(tst_perp))\n",
    "    # print(\"Training is over.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-lover",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "provincial-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_l(model, data, epochs, learning_rate, learning_rate_decay, max_grad):\n",
    "    \n",
    "    train_loader, valid_loader, test_loader = data\n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_words = 0\n",
    "    print(\"Starting training.\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batch_size = train_loader.batch_size\n",
    "        states = model.init_state(batch_size)\n",
    "\n",
    "        if epoch > 5:\n",
    "            learning_rate = learning_rate / learning_rate_decay\n",
    "        \n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            # if x.size(0) < batch_size:\n",
    "            #    continue\n",
    "            print(f\"x size before : {x.size()}\")\n",
    "            print(x)\n",
    "            print(y)\n",
    "            # x = torch.transpose(x, 0, 1)\n",
    "            print(f\"x size after : {x.size()}\")\n",
    "            # y = torch.transpose(y, 0, 1)\n",
    "            \n",
    "            total_words += x.numel()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # batch_size = len(x))\n",
    "            states = model.detach_states(states)\n",
    "            scores, states = model(x, states)\n",
    "            loss = neg_log_likelihood_loss(scores, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                norm = nn.utils.clip_grad_norm_(model.parameters(), max_grad)\n",
    "                for param in model.parameters():\n",
    "                    param -= learning_rate * param.grad\n",
    "            \n",
    "            if i % (len(train_loader) // 10) == 0:\n",
    "                end_time = time.time()\n",
    "                print(\"batch no = {:d} / {:d}, \".format(i, len(train_loader)) +\n",
    "                      \"train loss = {:.3f}, \".format(loss.item() / batch_size) +\n",
    "                      \"wps = {:d}, \".format(round(total_words/(end_time-start_time))) +\n",
    "                      \"dw.norm() = {:.3f}, \".format(norm) +\n",
    "                      \"lr = {:.3f}, \".format(learning_rate) +\n",
    "                      \"since beginning = {:d} mins, \".format(round((end_time-start_time)/60))) # +\n",
    "                      # \"cuda memory = {:.3f} GBs\".format(torch.cuda.max_memory_allocated()/1024/1024/1024))\n",
    "        \n",
    "        model.eval()\n",
    "        valid_perplexity = get_perplexity(model, valid_loader, batch_size)\n",
    "        print(\"Epoch : {:d} || Validation set perplexity : {:.3f}\".format(epoch+1, valid_perplexity))\n",
    "        print(\"*************************************************\\n\")\n",
    "    test_perp = get_perplexity(model, test_loader, batch_size)\n",
    "    print(\"Test set perplexity : {:.3f}\".format(test_perp))\n",
    "    print(\"Training is over.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "assigned-juvenile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dims = int(np.ceil(np.sqrt(np.sqrt(vocab_size))))\n",
    "embed_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "indie-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dims = int(np.ceil(np.sqrt(np.sqrt(vocab_size))))\n",
    "model = LSTM_Model(vocab_size=vocab_size, max_grad=5, embed_dims=embed_dims, num_layers=2,\n",
    "                   dropout_prob=0.5, init_param=0.05, bias=False, embed_tying=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "continent-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, vld, tst = data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "formed-analyst",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function _VariableFunctionsClass.transpose>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "employed-declaration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "\n",
      "x size before : torch.Size([3, 4, 1])\n",
      "tensor([[[ 5],\n",
      "         [21],\n",
      "         [29],\n",
      "         [13]],\n",
      "\n",
      "        [[18],\n",
      "         [ 5],\n",
      "         [ 5],\n",
      "         [25]],\n",
      "\n",
      "        [[ 5],\n",
      "         [25],\n",
      "         [ 9],\n",
      "         [ 2]]])\n",
      "tensor([[[21],\n",
      "         [29],\n",
      "         [13],\n",
      "         [26]],\n",
      "\n",
      "        [[ 5],\n",
      "         [ 5],\n",
      "         [25],\n",
      "         [ 5]],\n",
      "\n",
      "        [[25],\n",
      "         [ 9],\n",
      "         [ 2],\n",
      "         [ 4]]])\n",
      "x size after : torch.Size([3, 4, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-680cfb891186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-b7a2a40ee619>\u001b[0m in \u001b[0;36mtrain_l\u001b[0;34m(model, data, epochs, learning_rate, learning_rate_decay, max_grad)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# batch_size = len(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_log_likelihood_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_projects/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/MSAI/courses/nlp/Project #2/msai_337_project_2/lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, states)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_projects/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_projects/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_projects/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_projects/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mexpected_input_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_input_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    175\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[1;32m    176\u001b[0m                     expected_input_dim, input.dim()))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 4"
     ]
    }
   ],
   "source": [
    "train_l(model=model, data=data_loaders, epochs=5, learning_rate=1., learning_rate_decay=1.2, max_grad=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "christian-founder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "\n",
      "batch no = 0 / 3371, train loss = 10.272, wps = 2065, dw.norm() = 0.001, lr = 1.000, since beginning = 0 mins, \n",
      "batch no = 337 / 3371, train loss = 10.272, wps = 3510, dw.norm() = 0.001, lr = 1.000, since beginning = 1 mins, \n",
      "batch no = 674 / 3371, train loss = 10.272, wps = 3188, dw.norm() = 0.003, lr = 1.000, since beginning = 2 mins, \n",
      "batch no = 1011 / 3371, train loss = 7.877, wps = 3123, dw.norm() = 3.944, lr = 1.000, since beginning = 3 mins, \n",
      "batch no = 1348 / 3371, train loss = 7.830, wps = 3061, dw.norm() = 12.840, lr = 1.000, since beginning = 4 mins, \n",
      "batch no = 1685 / 3371, train loss = 7.678, wps = 3053, dw.norm() = 3.644, lr = 1.000, since beginning = 5 mins, \n",
      "batch no = 2022 / 3371, train loss = 7.355, wps = 3007, dw.norm() = 8.166, lr = 1.000, since beginning = 7 mins, \n",
      "batch no = 2359 / 3371, train loss = 7.253, wps = 2963, dw.norm() = 3.315, lr = 1.000, since beginning = 8 mins, \n",
      "batch no = 2696 / 3371, train loss = 7.279, wps = 2895, dw.norm() = 4.190, lr = 1.000, since beginning = 9 mins, \n",
      "batch no = 3033 / 3371, train loss = 7.264, wps = 2906, dw.norm() = 3.485, lr = 1.000, since beginning = 10 mins, \n",
      "batch no = 0 / 3371, train loss = 7.810, wps = 2900, dw.norm() = 8.337, lr = 1.000, since beginning = 11 mins, \n",
      "batch no = 337 / 3371, train loss = 7.148, wps = 2838, dw.norm() = 4.155, lr = 1.000, since beginning = 13 mins, \n",
      "batch no = 674 / 3371, train loss = 7.028, wps = 2808, dw.norm() = 2.771, lr = 1.000, since beginning = 14 mins, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-e614f2fde80b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-153ee2d7695a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, data, model, epochs, lr, max_grad, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_log_likelihood_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_projects/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/MSAI/courses/nlp/Project #2/msai_337_project_2/lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, states)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_projects/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/nlp_projects/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    582\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(device='cpu', data=data_loaders, model=model, epochs=5, lr=1., max_grad=5, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-bolivia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-laugh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-lincoln",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "advised-guinea",
   "metadata": {},
   "source": [
    "## SHAPE OF ARRAYS AND TENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "frequent-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import _init_corpora\n",
    "train, valid, test, vocab = _init_corpora(path=path, topic=topic, freq_threshold=freq_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "brief-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t = train.reshape(-1, 1)\n",
    "valid_t = valid.reshape(-1, 1)\n",
    "test_t = test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "experienced-kitty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "southern-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "arranged-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = torch.LongTensor(test)\n",
    "TEST_T = torch.LongTensor(test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "medical-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = torch.split(tensor=TEST, split_size_or_sections=time_steps)\n",
    "TEST_T = torch.split(tensor=TEST_T, split_size_or_sections=time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "reduced-athens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([31, 25,  5,  5,  5, 44,  5, 33,  5,  5]),\n",
       " tensor([40, 41,  5, 16,  5,  5,  5,  5,  5,  5]),\n",
       " tensor([ 2,  4,  5,  7,  5,  5, 33,  5,  5, 36]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "multiple-election",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[31],\n",
       "         [25],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [44],\n",
       "         [ 5],\n",
       "         [33],\n",
       "         [ 5],\n",
       "         [ 5]]),\n",
       " tensor([[40],\n",
       "         [41],\n",
       "         [ 5],\n",
       "         [16],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5]]),\n",
       " tensor([[ 2],\n",
       "         [ 4],\n",
       "         [ 5],\n",
       "         [ 7],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [33],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [36]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_T[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "coordinated-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_seq = pad_sequence(TEST, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "diverse-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_T_seq = pad_sequence(TEST_T, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "strong-resolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 40,  5,  5,  5,  5,  5, 33,  5,  5],\n",
       "        [ 8,  5, 10,  5,  5,  1,  5,  5,  5,  5],\n",
       "        [ 5,  5,  5,  5,  2,  4,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_seq[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "combined-archives",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5],\n",
       "         [40],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [33],\n",
       "         [ 5],\n",
       "         [ 5]],\n",
       "\n",
       "        [[ 8],\n",
       "         [ 5],\n",
       "         [10],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 1],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5]],\n",
       "\n",
       "        [[ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 2],\n",
       "         [ 4],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_T_seq[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "indoor-badge",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "start (1) + length (10) exceeds dimension size (10).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-a7d475e00407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTEST_seq_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEST_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTEST_seq_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEST_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: start (1) + length (10) exceeds dimension size (10)."
     ]
    }
   ],
   "source": [
    "TEST_seq_i = TEST_seq.narrow_copy(1, 0, TEST_seq.shape[1])\n",
    "TEST_seq_o = TEST_seq.narrow_copy(1, 1, TEST_seq.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cosmetic-magazine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 40,  5,  5,  5,  5,  5, 33,  5,  5],\n",
       "        [ 8,  5, 10,  5,  5,  1,  5,  5,  5,  5],\n",
       "        [ 5,  5,  5,  5,  2,  4,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_seq_i[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "musical-transfer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[40,  5,  5,  5,  5,  5, 33,  5,  5],\n",
       "        [ 5, 10,  5,  5,  1,  5,  5,  5,  5],\n",
       "        [ 5,  5,  5,  2,  4,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_seq_o[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bronze-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_T_seq_i = TEST_T_seq.narrow_copy(1, 0, TEST_T_seq.shape[1] - 1)\n",
    "TEST_T_seq_o = TEST_T_seq.narrow_copy(1, 1, TEST_T_seq.shape[1] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "approximate-delivery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[15502],\n",
       "         [ 7717],\n",
       "         [18542],\n",
       "         [ 7105],\n",
       "         [   77],\n",
       "         [26313],\n",
       "         [12434],\n",
       "         [23496],\n",
       "         [ 2866]],\n",
       "\n",
       "        [[10880],\n",
       "         [10291],\n",
       "         [ 9056],\n",
       "         [   77],\n",
       "         [13514],\n",
       "         [22559],\n",
       "         [ 2150],\n",
       "         [   77],\n",
       "         [ 9054]],\n",
       "\n",
       "        [[   77],\n",
       "         [ 9054],\n",
       "         [19242],\n",
       "         [   77],\n",
       "         [ 2378],\n",
       "         [ 9056],\n",
       "         [ 1419],\n",
       "         [   80],\n",
       "         [    0]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_T_seq_i[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "further-bathroom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7717],\n",
       "         [18542],\n",
       "         [ 7105],\n",
       "         [   77],\n",
       "         [26313],\n",
       "         [12434],\n",
       "         [23496],\n",
       "         [ 2866],\n",
       "         [17241]],\n",
       "\n",
       "        [[10291],\n",
       "         [ 9056],\n",
       "         [   77],\n",
       "         [13514],\n",
       "         [22559],\n",
       "         [ 2150],\n",
       "         [   77],\n",
       "         [ 9054],\n",
       "         [25836]],\n",
       "\n",
       "        [[ 9054],\n",
       "         [19242],\n",
       "         [   77],\n",
       "         [ 2378],\n",
       "         [ 9056],\n",
       "         [ 1419],\n",
       "         [   80],\n",
       "         [    0],\n",
       "         [    0]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_T_seq_o[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "funded-shooting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23773, 9])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_seq_i.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "excessive-hamburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23773, 9, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_T_seq_i.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.LongTensor(data)\n",
    "# split tensor into tensors of of size time_steps\n",
    "data = torch.split(tensor=data, split_size_or_sections=time_steps)\n",
    "\n",
    "# note: word2index['<pad>'] = 0\n",
    "sequences = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "\n",
    "# from seq we generate 2 copies.\n",
    "# inputs=seq[:-1], targets=seq[1:]\n",
    "sequences_inputs = sequences.narrow_copy(1, 0, sequences.shape[1] - 1)\n",
    "sequences_targets = sequences.narrow_copy(1, 1, sequences.shape[1] - 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
