{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extensive-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTM_Model\n",
    "from data import init_datasets\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "disabled-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood_loss(scores, targets):\n",
    "    # substituting with cross entropy loss\n",
    "    # get batch size\n",
    "    batch_size = targets.size(1)\n",
    "\n",
    "    # print(f\"scores size : {scores.size()}\")\n",
    "    # print(f\"scores reshaped to : {scores.reshape(-1, scores.size(2)).size()}\")\n",
    "\n",
    "    # print(f\"targets size : {targets.size()}\")\n",
    "    # print(f\"targets reshaped to : {targets.reshape(-1).size()}\")\n",
    "\n",
    "    # scores are shape (batch_size, time_steps, vocab_size)\n",
    "    # scores are reshaped to (batch_size * time_steps, vocab_size)\n",
    "\n",
    "    # targets are shape (batch_size, time_steps)\n",
    "    # targets are reshapes to (batch_size*time_steps)\n",
    "    return F.cross_entropy(scores.reshape(-1, scores.size(2)), targets.reshape(-1)) * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "armed-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(data, model, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        states = model.init_state(batch_size)\n",
    "        for x, y in data:\n",
    "            # print(f\"x size : {x.size()}\")\n",
    "            # print(f\"y size : {y.size()}\")\n",
    "            scores, states = model(x, states)\n",
    "            # print(f\"scores size : {scores.size()}\")\n",
    "            loss = neg_log_likelihood_loss(scores, y)\n",
    "            # print(f\"loss : {loss}\")\n",
    "            #Again with the sum/average implementation described in 'nll_loss'.\n",
    "            losses.append(loss.data.item() / batch_size)\n",
    "    return np.exp(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "several-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, model, epochs, learning_rate, learning_rate_decay, max_grad):\n",
    "    \n",
    "    train_loader, valid_loader, test_loader = data\n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_words = 0\n",
    "    print(\"Starting training.\\n\")\n",
    "    batch_size = train_loader.batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # batch_size = train_loader.batch_size\n",
    "        states = model.init_state(batch_size)\n",
    "\n",
    "        if epoch > 5:\n",
    "            learning_rate = learning_rate / learning_rate_decay\n",
    "        \n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            total_words += x.numel()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # batch_size = len(x))\n",
    "            states = model.detach_states(states)\n",
    "            scores, states = model(x, states)\n",
    "            loss = neg_log_likelihood_loss(scores=scores, targets=y)\n",
    "            loss.backward()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                norm = nn.utils.clip_grad_norm_(model.parameters(), max_grad)\n",
    "                for param in model.parameters():\n",
    "                    param -= learning_rate * param.grad\n",
    "            \n",
    "            if i % (len(train_loader) // 10) == 0:\n",
    "                end_time = time.time()\n",
    "                print(\"batch no = {:d} / {:d}, \".format(i, len(train_loader)) +\n",
    "                      \"train loss = {:.3f}, \".format(loss.item() / batch_size) +\n",
    "                      \"wps = {:d}, \".format(round(total_words/(end_time-start_time))) +\n",
    "                      \"dw.norm() = {:.3f}, \".format(norm) +\n",
    "                      \"lr = {:.3f}, \".format(learning_rate) +\n",
    "                      \"since beginning = {:d} mins, \".format(round((end_time-start_time)/60))) # +\n",
    "                      # \"cuda memory = {:.3f} GBs\".format(torch.cuda.max_memory_allocated()/1024/1024/1024))\n",
    "        \n",
    "        model.eval()\n",
    "        valid_perplexity = get_perplexity(data=valid_loader, model=model, batch_size=batch_size)\n",
    "        print(\"Epoch : {:d} || Validation set perplexity : {:.3f}\".format(epoch+1, valid_perplexity))\n",
    "        print(\"*************************************************\\n\")\n",
    "    test_perp = get_perplexity(data=test_loader, model=model, batch_size=batch_size)\n",
    "    print(\"Test set perplexity : {:.3f}\".format(test_perp))\n",
    "    print(\"Training is over.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "decent-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "        'embed_dims': 25,\n",
    "        'device': 'cpu', # 'gpu'\n",
    "        'freq_threshold': 1,\n",
    "        'dropout_prob': 0.5,\n",
    "        'init_range': 0.05,\n",
    "        'epochs': 10,\n",
    "        'learning_rate': 1,\n",
    "        'learning_rate_decay': 1.2,\n",
    "        'num_layers': 2,\n",
    "        'batch_size': 5, #TODO: on nyt small dataset, we get issues with batch size and timesteps. Not sure why\n",
    "        'time_steps': 10,\n",
    "        'max_grad': 5,\n",
    "        'embed_tying': False,\n",
    "        'bias': False,\n",
    "        'save_model': True,\n",
    "        'load_model': False,\n",
    "        'model_path': 'lstm_model',\n",
    "        'topic': 'nyt_covid', # enter 'wiki' or 'nyt_covid'\n",
    "        'path': 'data/test_corpora'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "atmospheric-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params for init_datasets\n",
    "data_params = {k:hyperparams[k] for k in ['topic','freq_threshold', 'time_steps', 'batch_size',  'path']}\n",
    "datasets = init_datasets(**data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "parallel-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we store the vcab size and word2index dict\n",
    "vocab_size = datasets['vocab_size']\n",
    "word2index = datasets['word2index']\n",
    "\n",
    "# we get the data_loaders: train, valid, test\n",
    "data_loaders = datasets['data_loaders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "operating-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params for model training\n",
    "model_params = ['device', 'embed_dims', 'dropout_prob', 'init_range',\n",
    "                'num_layers', 'max_grad', 'embed_tying', 'bias']\n",
    "model_params = {k:hyperparams[k] for k in model_params}\n",
    "model_params['vocab_size'] = vocab_size\n",
    "model = LSTM_Model(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-projector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "greatest-notification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader = data_loaders\n",
    "start_time = time.time()\n",
    "\n",
    "total_words = 0\n",
    "print(\"Starting training.\\n\")\n",
    "batch_size = train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "closing-composite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('dropout', Dropout(p=0.5, inplace=False)),\n",
       "              ('embed', Embedding(1085, 25)),\n",
       "              ('lstms',\n",
       "               ModuleList(\n",
       "                 (0): LSTM(25, 25, bias=False, batch_first=True)\n",
       "                 (1): LSTM(25, 25, bias=False, batch_first=True)\n",
       "               )),\n",
       "              ('fc', Linear(in_features=25, out_features=1085, bias=False))]),\n",
       " 'max_grad': 5,\n",
       " 'init_param': 0.05,\n",
       " 'bias': False,\n",
       " 'embed_tying': False,\n",
       " 'vocab_size': 1085,\n",
       " 'device': device(type='cpu')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mobile-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = model.init_state(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "classical-uniform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 25])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fitted-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = get_perplexity(data=train_loader, model=model, batch_size=train_loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "southeast-element",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114563.8136259254"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "demographic-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "states = model.init_state(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "combined-bench",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7ff66c34adc0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "environmental-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "increased-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "japanese-shield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 10, 41,  5,  5],\n",
      "        [ 5,  1,  7,  5,  5],\n",
      "        [21, 29, 38,  1, 41]])\n",
      "tensor([[10, 41,  5,  5,  5],\n",
      "        [ 1,  7,  5,  5,  5],\n",
      "        [29, 38,  1, 41, 24]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "median-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, states = model(x, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "upper-patrick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "torch.Size([3, 5, 1085])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(scores.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "quantitative-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = neg_log_likelihood_loss(scores, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "answering-herald",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(34.9466, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "respiratory-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "expired-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses.append(loss.data.item() / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "flexible-calvin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.648880004882812]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "loved-elements",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.648880004882812"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "light-demand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114562.98119796545"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ready-corner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = y.size(1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "physical-median",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood_loss(scores, targets):\n",
    "    # substituting with cross entropy loss\n",
    "    # get batch size\n",
    "    (batch_size, seq length)\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    # print(f\"scores size : {scores.size()}\")\n",
    "    # print(f\"scores reshaped to : {scores.reshape(-1, scores.size(2)).size()}\")\n",
    "\n",
    "    # print(f\"targets size : {targets.size()}\")\n",
    "    # print(f\"targets reshaped to : {targets.reshape(-1).size()}\")\n",
    "\n",
    "    # scores are shape (batch_size, time_steps, vocab_size)\n",
    "    # scores are reshaped to (batch_size * time_steps, vocab_size)\n",
    "\n",
    "    # targets are shape (batch_size, time_steps)\n",
    "    # targets are reshapes to (batch_size*time_steps)\n",
    "    return F.cross_entropy(scores.reshape(-1, scores.size(2)), targets.reshape(-1)) * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-findings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-laugh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-september",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        losses = []\n",
    "        states = model.init_state(batch_size)\n",
    "        for x, y in data:\n",
    "            # print(f\"x size : {x.size()}\")\n",
    "            # print(f\"y size : {y.size()}\")\n",
    "            scores, states = model(x, states)\n",
    "            # print(f\"scores size : {scores.size()}\")\n",
    "            loss = neg_log_likelihood_loss(scores, y)\n",
    "            # print(f\"loss : {loss}\")\n",
    "            #Again with the sum/average implementation described in 'nll_loss'.\n",
    "            losses.append(loss.data.item() / batch_size)\n",
    "    return np.exp(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-trustee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-allergy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-brush",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-adapter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    # batch_size = train_loader.batch_size\n",
    "    states = model.init_state(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-netherlands",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "terminal-easter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main():\n",
    "    hyperparams = {\n",
    "        'embed_dims': 25,\n",
    "        'device': 'cpu', # 'gpu'\n",
    "        'freq_threshold': 1,\n",
    "        'dropout_prob': 0.5,\n",
    "        'init_range': 0.05,\n",
    "        'epochs': 10,\n",
    "        'learning_rate': 1,\n",
    "        'learning_rate_decay': 1.2,\n",
    "        'num_layers': 2,\n",
    "        'batch_size': 5, #TODO: on nyt small dataset, we get issues with batch size and timesteps. Not sure why\n",
    "        'time_steps': 10,\n",
    "        'max_grad': 5,\n",
    "        'embed_tying': False,\n",
    "        'bias': False,\n",
    "        'save_model': True,\n",
    "        'load_model': False,\n",
    "        'model_path': 'lstm_model',\n",
    "        'topic': 'nyt_covid', # enter 'wiki' or 'nyt_covid'\n",
    "        'path': 'data/test_corpora'\n",
    "    }\n",
    "\n",
    "    # set params for init_datasets\n",
    "    data_params = {k:hyperparams[k] for k in ['topic','freq_threshold', 'time_steps', 'batch_size',  'path']}\n",
    "    datasets = init_datasets(**data_params)\n",
    "\n",
    "    # we store the vcab size and word2index dict\n",
    "    vocab_size = datasets['vocab_size']\n",
    "    word2index = datasets['word2index']\n",
    "\n",
    "    # we get the data_loaders: train, valid, test\n",
    "    data_loaders = datasets['data_loaders']\n",
    "\n",
    "    # set params for model training\n",
    "    model_params = ['device', 'embed_dims', 'dropout_prob', 'init_range',\n",
    "                    'num_layers', 'max_grad', 'embed_tying', 'bias']\n",
    "    model_params = {k:hyperparams[k] for k in model_params}\n",
    "    model_params['vocab_size'] = vocab_size\n",
    "    # Masum recommended this as embed dims\n",
    "    # TODO: Make this more easily modifiable.\n",
    "    #   Want to do embed dims = user input if input provied, else embed dims = line below\n",
    "    # model_params['embed_dims'] = int(np.ceil(np.sqrt(np.sqrt(vocab_size))))\n",
    "    model = LSTM_Model(**model_params)\n",
    "    print(f\"vocab size : {vocab_size}\")\n",
    "    perplexity = get_perplexity(data=data_loaders[1], model=model, batch_size=data_loaders[1].batch_size)\n",
    "    print(\"perplexity on %s dataset before training: %.3f, \" % ('valid', perplexity))\n",
    "    \"\"\"\n",
    "    for d, l in zip(data_loaders, ['train', 'valid', 'test']):\n",
    "        perplexity = get_perplexity(data=d, model=model, batch_size=d.batch_size)\n",
    "        print(\"perplexity on %s dataset before training: %.3f, \" % (l, perplexity))\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if hyperparams['load_model']:\n",
    "        model.load_state_dict(torch.load(hyperparams['model_path']))\n",
    "\n",
    "    else:\n",
    "        training_params = ['epochs', 'learning_rate', 'learning_rate_decay', 'max_grad']\n",
    "        training_params = {k:hyperparams[k] for k in training_params}\n",
    "        model = train(data=data_loaders, model=model, **training_params)\n",
    "\n",
    "    # now calculate perplexities for train, valid, test\n",
    "    for d, l in zip(data_loaders, ['train', 'valid', 'test']):\n",
    "        perplexity = get_perplexity(data=d, model=model, batch_size=d.batch_size)\n",
    "        print(\"perplexity on %s dataset after training : %.3f, \" % (l, perplexity))\n",
    "\n",
    "    if hyperparams['save_model']:\n",
    "        torch.save(model.state_dict(), hyperparams['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deadly-airline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 4, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dataset.x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fossil-panel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[38],\n",
       "        [13],\n",
       "        [26]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sixth-aquarium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13],\n",
       "        [26],\n",
       "        [ 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1][0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "positive-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'embed_dims': None,\n",
    "    'freq_threshold': 3,\n",
    "    'dropout_prob': 0.5,\n",
    "    'init_range': 0.05,\n",
    "    'epochs': 40,\n",
    "    'learning_rate': 1,\n",
    "    'learning_rate_decay': 1.2,\n",
    "    'num_layers': 2,\n",
    "    'batch_size': 20,\n",
    "    'time_steps': 35,\n",
    "    'max_grad': 5,\n",
    "    'embed_tying': False,\n",
    "    'bias': False,\n",
    "    'save_model': True,\n",
    "    'load_model': True,\n",
    "    'model_path': 'lstm_model',\n",
    "    'topic': 'wiki', # enter 'wiki' or 'nyt_covid'\n",
    "    'path': 'data/small_test_corpora'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-frost",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-framing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-latest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-nancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "impaired-leisure",
   "metadata": {},
   "source": [
    "## SHAPE OF ARRAYS AND TENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "possible-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import _init_corpora\n",
    "train, valid, test, vocab = _init_corpora(path=path, topic=topic, freq_threshold=freq_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "economic-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t = train.reshape(-1, 1)\n",
    "valid_t = valid.reshape(-1, 1)\n",
    "test_t = test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "equal-latin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "exempt-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "willing-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = torch.LongTensor(test)\n",
    "TEST_T = torch.LongTensor(test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cathedral-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = torch.split(tensor=TEST, split_size_or_sections=time_steps)\n",
    "TEST_T = torch.split(tensor=TEST_T, split_size_or_sections=time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "exterior-alias",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([31, 25,  5,  5,  5, 44,  5, 33,  5,  5]),\n",
       " tensor([40, 41,  5, 16,  5,  5,  5,  5,  5,  5]),\n",
       " tensor([ 2,  4,  5,  7,  5,  5, 33,  5,  5, 36]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "republican-muslim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[31],\n",
       "         [25],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [44],\n",
       "         [ 5],\n",
       "         [33],\n",
       "         [ 5],\n",
       "         [ 5]]),\n",
       " tensor([[40],\n",
       "         [41],\n",
       "         [ 5],\n",
       "         [16],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5]]),\n",
       " tensor([[ 2],\n",
       "         [ 4],\n",
       "         [ 5],\n",
       "         [ 7],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [33],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [36]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_T[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "descending-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_seq = pad_sequence(TEST, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "sticky-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_T_seq = pad_sequence(TEST_T, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "altered-administration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 40,  5,  5,  5,  5,  5, 33,  5,  5],\n",
       "        [ 8,  5, 10,  5,  5,  1,  5,  5,  5,  5],\n",
       "        [ 5,  5,  5,  5,  2,  4,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_seq[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "satisfactory-helen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5],\n",
       "         [40],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [33],\n",
       "         [ 5],\n",
       "         [ 5]],\n",
       "\n",
       "        [[ 8],\n",
       "         [ 5],\n",
       "         [10],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 1],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5]],\n",
       "\n",
       "        [[ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 5],\n",
       "         [ 2],\n",
       "         [ 4],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0],\n",
       "         [ 0]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_T_seq[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "blond-breathing",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "start (1) + length (10) exceeds dimension size (10).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-a7d475e00407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTEST_seq_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEST_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTEST_seq_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEST_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: start (1) + length (10) exceeds dimension size (10)."
     ]
    }
   ],
   "source": [
    "TEST_seq_i = TEST_seq.narrow_copy(1, 0, TEST_seq.shape[1])\n",
    "TEST_seq_o = TEST_seq.narrow_copy(1, 1, TEST_seq.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "palestinian-maintenance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 40,  5,  5,  5,  5,  5, 33,  5,  5],\n",
       "        [ 8,  5, 10,  5,  5,  1,  5,  5,  5,  5],\n",
       "        [ 5,  5,  5,  5,  2,  4,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_seq_i[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "egyptian-logic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[40,  5,  5,  5,  5,  5, 33,  5,  5],\n",
       "        [ 5, 10,  5,  5,  1,  5,  5,  5,  5],\n",
       "        [ 5,  5,  5,  2,  4,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_seq_o[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "mechanical-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_T_seq_i = TEST_T_seq.narrow_copy(1, 0, TEST_T_seq.shape[1] - 1)\n",
    "TEST_T_seq_o = TEST_T_seq.narrow_copy(1, 1, TEST_T_seq.shape[1] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "prime-notebook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[15502],\n",
       "         [ 7717],\n",
       "         [18542],\n",
       "         [ 7105],\n",
       "         [   77],\n",
       "         [26313],\n",
       "         [12434],\n",
       "         [23496],\n",
       "         [ 2866]],\n",
       "\n",
       "        [[10880],\n",
       "         [10291],\n",
       "         [ 9056],\n",
       "         [   77],\n",
       "         [13514],\n",
       "         [22559],\n",
       "         [ 2150],\n",
       "         [   77],\n",
       "         [ 9054]],\n",
       "\n",
       "        [[   77],\n",
       "         [ 9054],\n",
       "         [19242],\n",
       "         [   77],\n",
       "         [ 2378],\n",
       "         [ 9056],\n",
       "         [ 1419],\n",
       "         [   80],\n",
       "         [    0]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_T_seq_i[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "funded-glossary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7717],\n",
       "         [18542],\n",
       "         [ 7105],\n",
       "         [   77],\n",
       "         [26313],\n",
       "         [12434],\n",
       "         [23496],\n",
       "         [ 2866],\n",
       "         [17241]],\n",
       "\n",
       "        [[10291],\n",
       "         [ 9056],\n",
       "         [   77],\n",
       "         [13514],\n",
       "         [22559],\n",
       "         [ 2150],\n",
       "         [   77],\n",
       "         [ 9054],\n",
       "         [25836]],\n",
       "\n",
       "        [[ 9054],\n",
       "         [19242],\n",
       "         [   77],\n",
       "         [ 2378],\n",
       "         [ 9056],\n",
       "         [ 1419],\n",
       "         [   80],\n",
       "         [    0],\n",
       "         [    0]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_T_seq_o[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "australian-density",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23773, 9])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_seq_i.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "conventional-individual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23773, 9, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_T_seq_i.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.LongTensor(data)\n",
    "# split tensor into tensors of of size time_steps\n",
    "data = torch.split(tensor=data, split_size_or_sections=time_steps)\n",
    "\n",
    "# note: word2index['<pad>'] = 0\n",
    "sequences = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "\n",
    "# from seq we generate 2 copies.\n",
    "# inputs=seq[:-1], targets=seq[1:]\n",
    "sequences_inputs = sequences.narrow_copy(1, 0, sequences.shape[1] - 1)\n",
    "sequences_targets = sequences.narrow_copy(1, 1, sequences.shape[1] - 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
